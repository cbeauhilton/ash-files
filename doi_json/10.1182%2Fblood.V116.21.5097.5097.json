{
    "doi": "https://doi.org/10.1182/blood.V116.21.5097.5097",
    "start_url": "https://ashpublications.org/blood/search-results?sort=Date+-+Oldest+First&f_ArticleTypeDisplayName=Meeting+Report&fl_SiteID=1000001&page=1706",
    "start_url_page_num": 1706,
    "is_scraped": "1",
    "article_title": "Intra-Reader Variability In Identifying Centroblast Cells From Digital Follicular Lymphoma Cases ",
    "article_date": "November 19, 2010",
    "session_type": "Non-Hodgkin Lymphoma - Biology, excluding Therapy",
    "topics": [
        "centroblasts",
        "follicular lymphoma",
        "chemoprevention",
        "lymphoma",
        "medical pathologists",
        "computer mouse",
        "grading system",
        "magnification",
        "software",
        "world health organization"
    ],
    "author_names": [
        "Kamel Belkacem-Boussaid",
        "Michael Pennell",
        "Ah Arwa Shana'",
        "Amy Gerwitz",
        "Weiqiang Zhao",
        "Frederick Racke, MD, PhD",
        "Eric Hsi, MD",
        "Gerard Lozanski, MD",
        "Metin Gurcan"
    ],
    "author_affiliations": [
        [
            "Biomedical Informatics, Ohio State Uiniversity, Columbus, OH, USA, "
        ],
        [
            "Biostatistics, Ohio State University, Columbus, OH, USA, "
        ],
        [
            "Pathology, Ohio State University, Columbus, OH, USA, "
        ],
        [
            "Pathology, Ohio State University, Columbus, OH, USA, "
        ],
        [
            "Pathology, Ohio State University, Columbus, OH, USA, "
        ],
        [
            "Ohio State University, Dublin, OH, USA, "
        ],
        [
            "Cleveland Clinic Foundation, Cleveland, OH, USA, "
        ],
        [
            "Pathology, The Ohio State University Medical Center, "
        ],
        [
            "Biomedical Informatics, Ohio State University, Columbus, OH, USA"
        ]
    ],
    "first_author_latitude": "39.9938966",
    "first_author_longitude": "-83.0162045",
    "abstract_text": "Abstract 5097 Method The goal of this research is to assess intra-reader variability in identifying centroblast (CB) cells from digitized H&E-stained Follicular Lymphoma (FL) cases. We have enrolled three board-certified hematopathologists experienced in FL grading to complete two reading sessions on 51 High Power Field (HPF: 40 \u00d7 magnification) images. These images were selected randomly from a database of five hundred HPF images collected from 17 different patients. The dataset is comprised of lymphoma cases with different grades FL and all grades (1, 2, and 3) are represented. Each pathologist was asked to grade the same set of images (51 images, 3 per patient) on different days (with a minimum of two month intervals). In the second reading the order of the images were also randomized. In each reading session, the pathologists examined digital images and recorded the spatial coordinates of CBs using in-house built marking software that allowed pathologists to mark CB cells using only their computer mouse. Experimental Results The results from each reading session were analyzed in terms of FL grade which was determined by averaging the centroblast counts across the three images for a patient and assigning grade using the standard WHO guidelines: Grade I = 0\u20135, Grade II = 6\u201315, Grade III = > 15 centroblasts/image. Two different kappa statistics were used to measure agreement. First, we used a weighted kappa in order to measure intra-reader agreement on the three level grade and then we computed a simple kappa measuring agreement on a two level diagnosis: Grade I or II (no chemoprevention assigned) versus Grade III (chemoprevention assigned). Weighted Kappa for Three Level Grade  Pathologist . Kappa . p-value . 1 0.4295 0.0824 2 0.4688 0.0515 3 1 0.0015 Mean 0.6328  Pathologist . Kappa . p-value . 1 0.4295 0.0824 2 0.4688 0.0515 3 1 0.0015 Mean 0.6328  View Large Kappa for Two Level Diagnosis (Grade I or II vs. III)  Pathologist . Kappa . p-value . 1 0.4295 0.0824 2 0.4688 0.0515 3 1 0.0015 Mean 0.6328  Pathologist . Kappa . p-value . 1 0.4295 0.0824 2 0.4688 0.0515 3 1 0.0015 Mean 0.6328  Landis and Koch [1] guidelines for degree of Kappa agreement; < 0 poor, 0\u20130.2 slight, 0.4\u20130.6 moderate, 0.6\u20130.8 substantial, 0.81\u20131 almost perfect. View Large Discussion Table 1 provides the weighted kappa statistics based on the three level grading system. There was statistically significant agreement for each pathologist with pathologist 3 exhibiting nearly perfect agreement in grade while pathologists 1 and 2 exhibited moderate agreement. However, when we examined agreement based on the clinically significant diagnosis (Grade I or II versus III) (see table 2 ), the kappa statistics for pathologists for 1 and 2 were not significantly different from zero suggesting that there is not significant agreement in their diagnoses. Pathologist 3, on the other hand, exhibited perfect agreement in the two level diagnosis across readings. Conclusion In this study, we have examined intra-reader variability in grading follicular lymphoma in digital images. Although similar studies have been conducted to measure the variability at the slide level [2], this is the first time we are reviewing the variability in CB detection. A larger data set will be considered in the near future to generalize the results. Reference 1. J. R. Landis and G. G. Koch (1977) \u201cThe measurement of observer agreement for categorical data\u201d in Biometrics, 1977, vol. 33, pp. 159\u2013174 2. G. E. Metter, B.N. Nathwani, J.S. Burke, et al, \u201cMorphological subclassification of follicular lymphoma: variability of diagnoses among hematopathologists, a collaborative study between the Repository Center and Pathology Panel for Lymphoma Clinical Studies\u201d, J. Clin Oncol. 1985: 3(1): 25\u201338. Disclosures: No relevant conflicts of interest to declare."
}